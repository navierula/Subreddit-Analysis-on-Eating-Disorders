{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load a sample of the raw JSON data into pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archived',\n",
       " 'author',\n",
       " 'author_flair_css_class',\n",
       " 'author_flair_text',\n",
       " 'body',\n",
       " 'controversiality',\n",
       " 'created_utc',\n",
       " 'distinguished',\n",
       " 'downs',\n",
       " 'edited',\n",
       " 'gilded',\n",
       " 'id',\n",
       " 'link_id',\n",
       " 'name',\n",
       " 'parent_id',\n",
       " 'retrieved_on',\n",
       " 'score',\n",
       " 'score_hidden',\n",
       " 'subreddit',\n",
       " 'subreddit_id',\n",
       " 'ups']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "json_file = 'sample_data'\n",
    "list(pd.read_json(json_file, lines=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform the full JSON file into a CSV, removing any stuff that we won't need:\n",
    "* `[deleted]` users or comments\n",
    "* comments with <10 tokens\n",
    "\n",
    "(*WARNING*: this takes ~2.5 hours) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "MIN_NUM_WORD_TOKENS = 10\n",
    "TOTAL_NUM_LINES = 53851542  # $ wc -l data_full.json \n",
    "PBAR_UPDATE_SIZE = 10000\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def _ok_to_write(entries):\n",
    "    if entries['author'] == '[deleted]':\n",
    "        return False\n",
    "    if entries['body'] == '[deleted]' or len(tokenizer.tokenize(entries['body'])) < MIN_NUM_WORD_TOKENS:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "out_columns = [\n",
    "    'author',\n",
    "    'body',\n",
    "    'subreddit',\n",
    "    'subreddit_id',\n",
    "    'score',\n",
    "]\n",
    "in_filename = 'data_full.json'\n",
    "out_filename = 'data_full_preprocessed.csv'\n",
    "count = 0\n",
    "pbar = tqdm(total=TOTAL_NUM_LINES)\n",
    "with open(out_filename, 'w') as o:\n",
    "    writer = csv.DictWriter(o, fieldnames=out_columns, extrasaction='ignore', \n",
    "                            delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writeheader()\n",
    "    with open(in_filename, 'r') as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            if count % PBAR_UPDATE_SIZE == 0:\n",
    "                pbar.update(PBAR_UPDATE_SIZE)\n",
    "            entries = json.loads(line)\n",
    "            if _ok_to_write(entries):\n",
    "                writer.writerow(entries)\n",
    "print('Done. Processed {} lines total.'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creates CSVs of text from comments made by users who have posted about anorexia or obesity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:09,  9.45s/it]\n",
      "3636it [11:22:37,  7.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # chunks processed: 3636.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create synonym sets for obesity and anorexia\n",
    "def syn_set(word_list):\n",
    "    syns = set()\n",
    "    for word in word_list:\n",
    "        for synset in wordnet.synsets(word):\n",
    "            for lemma in synset.lemmas():\n",
    "                syns.add(lemma.name())\n",
    "    return syns    \n",
    "\n",
    "OBESITY_SYNS = syn_set(['obesity'])\n",
    "ANOREXIA_SYNS = syn_set(['anorexia'])\n",
    "\n",
    "def row_filter_fn(df, syns):\n",
    "    \"\"\"Returns True if the row should be included, False otherwise.\"\"\"\n",
    "    # Check if any synonyms can be found.\n",
    "    if set([wordnet_lemmatizer.lemmatize(token.lower()) for token in tokenizer.tokenize(df)]) & syns:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "csv_filename = 'data_full_preprocessed.csv'\n",
    "chunksize = 10000\n",
    "count = 0\n",
    "obesity_data_frames = []\n",
    "anorexia_data_frames = []\n",
    "for chunk in tqdm(pd.read_csv(csv_filename, chunksize=chunksize)):\n",
    "    obesity_df = chunk[chunk['body'].apply(row_filter_fn, syns=OBESITY_SYNS)]\n",
    "    if not obesity_df.empty:\n",
    "        obesity_data_frames.append(obesity_df)\n",
    "    anorexia_df = chunk[chunk['body'].apply(row_filter_fn, syns=ANOREXIA_SYNS)]\n",
    "    if not anorexia_df.empty:\n",
    "        anorexia_data_frames.append(anorexia_df)\n",
    "    count += 1\n",
    "    #if count == 100: break\n",
    "print('Total # chunks processed: {}.'.format(count))\n",
    "\n",
    "# Write out to CSVs.\n",
    "pd.concat(obesity_data_frames).to_csv('obesity.csv', index=False)\n",
    "pd.concat(anorexia_data_frames).to_csv('anorexia.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
