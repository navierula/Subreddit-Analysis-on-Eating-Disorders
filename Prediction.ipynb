{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load datasets into Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "anorexiaSubreddits = pd.read_csv(\"data/subreddits_anorexia.csv\", encoding='ISO-8859-1')\n",
    "obesitySubreddits = pd.read_csv(\"data/subreddits_obesity.csv\", encoding='ISO-8859-1')\n",
    "bothSubreddits = pd.read_csv(\"data/subreddits_both.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract authors for each class (use hashes instead of usernames to protect privacy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "anorexia_authors = anorexiaSubreddits.drop_duplicates(subset=\"author\")['author'].apply(lambda a : hashlib.md5(a.encode()).hexdigest()).to_frame()\n",
    "obesity_authors = obesitySubreddits.drop_duplicates(subset=\"author\")['author'].apply(lambda a : hashlib.md5(a.encode()).hexdigest()).to_frame()\n",
    "both_authors = bothSubreddits.drop_duplicates(subset=\"author\")['author'].apply(lambda a : hashlib.md5(a.encode()).hexdigest()).to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anorexia_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3636it [05:42, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # chunks processed: 3636.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "csv_filename = '../../data_full_preprocessed.csv'\n",
    "chunksize = 10000\n",
    "count = 0\n",
    "obesity_author_data_frames = []\n",
    "anorexia_author_data_frames = []\n",
    "neither_author_data_frames = []\n",
    "\n",
    "anorexia_record_count = 0\n",
    "obesity_record_count = 0\n",
    "neither_record_count = 0\n",
    "\n",
    "for chunk in tqdm(pd.read_csv(csv_filename, chunksize=chunksize)):\n",
    "    chunk['author'] = chunk['author'].apply(lambda a : hashlib.md5(a.encode()).hexdigest())\n",
    "    anorexia_df = anorexia_authors.join(chunk.set_index('author'), on='author', how='inner', lsuffix='_left', rsuffix='_right')\n",
    "    if anorexia_record_count < 10000 and not anorexia_df.empty:\n",
    "        anorexia_author_data_frames.append(anorexia_df)\n",
    "        anorexia_record_count += len(anorexia_df)\n",
    "        \n",
    "    obesity_df = obesity_authors.join(chunk.set_index('author'), on='author', how='inner', lsuffix='_left', rsuffix='_right')\n",
    "    if obesity_record_count < 10000 and not obesity_df.empty:\n",
    "        obesity_author_data_frames.append(obesity_df)\n",
    "        obesity_record_count += len(obesity_df)\n",
    "        \n",
    "    # Use an outer join to get comments from users who have not posted about anorexia/obesity.\n",
    "    neither_df = chunk.join(both_authors, on='author', how='outer', lsuffix='_left', rsuffix='_right')\n",
    "    neither_df = neither_df[neither_df['author_right'].isnull()]\n",
    "    if neither_record_count < 10000 and not neither_df.empty:\n",
    "        neither_author_data_frames.append(neither_df)\n",
    "        neither_record_count += len(neither_df)\n",
    "        \n",
    "    count += 1\n",
    "    if anorexia_record_count > 10000 and obesity_record_count > 10000 and neither_record_count > 10000:\n",
    "        break\n",
    "print('Total # chunks processed: {}.'.format(count))\n",
    "\n",
    "pd.concat(anorexia_author_data_frames).to_csv('data/anorexia_author_data.csv', index=False)\n",
    "pd.concat(obesity_author_data_frames).to_csv('data/obesity_author_data.csv', index=False)\n",
    "pd.concat(neither_author_data_frames).to_csv('data/neither_author_data.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                author                       author_left  \\\n",
       " 0     49d264a69d92ec57c908cdb64cb30931  49d264a69d92ec57c908cdb64cb30931   \n",
       " 1     897ed4b79ba31c80057c87183b1cdb6e  897ed4b79ba31c80057c87183b1cdb6e   \n",
       " 2668  897ed4b79ba31c80057c87183b1cdb6e  897ed4b79ba31c80057c87183b1cdb6e   \n",
       " 2     f0d692685eeb47e5a22c164823c62295  f0d692685eeb47e5a22c164823c62295   \n",
       " 6971  f0d692685eeb47e5a22c164823c62295  f0d692685eeb47e5a22c164823c62295   \n",
       " 3     5e76a511987b00def6195687a3277fce  5e76a511987b00def6195687a3277fce   \n",
       " 4     0878fce284db4a1bbe9ffc633ec2c4fa  0878fce284db4a1bbe9ffc633ec2c4fa   \n",
       " 5     a4b6d677c71075a8a02908f2da08960e  a4b6d677c71075a8a02908f2da08960e   \n",
       " 6     e0676b7efcc36f86969db914b333f2be  e0676b7efcc36f86969db914b333f2be   \n",
       " 2330  e0676b7efcc36f86969db914b333f2be  e0676b7efcc36f86969db914b333f2be   \n",
       " 8845  e0676b7efcc36f86969db914b333f2be  e0676b7efcc36f86969db914b333f2be   \n",
       " 7     e14b0b4146749b77015bc7424f9d8c20  e14b0b4146749b77015bc7424f9d8c20   \n",
       " 2691  e14b0b4146749b77015bc7424f9d8c20  e14b0b4146749b77015bc7424f9d8c20   \n",
       " 8     4759cc28fed9f48ad48b480b818fa80f  4759cc28fed9f48ad48b480b818fa80f   \n",
       " 9     3d7f302c84406db9a5c018c348b7bb2f  3d7f302c84406db9a5c018c348b7bb2f   \n",
       " 10    d5354056b05e5234127a151e4c7d9454  d5354056b05e5234127a151e4c7d9454   \n",
       " 11    19ad371482e0c8c61cad30eabb375cfa  19ad371482e0c8c61cad30eabb375cfa   \n",
       " 12    8eed56f2e2cbe57be481781023bef7b7  8eed56f2e2cbe57be481781023bef7b7   \n",
       " 13    6e06193724a67f4c68e163b3e0edda03  6e06193724a67f4c68e163b3e0edda03   \n",
       " 5992  6e06193724a67f4c68e163b3e0edda03  6e06193724a67f4c68e163b3e0edda03   \n",
       " 14    d1a1fe354a76da47d16cbf98fcad560b  d1a1fe354a76da47d16cbf98fcad560b   \n",
       " 15    33be2a8eb28b541f8edeb0e8bcaa39b2  33be2a8eb28b541f8edeb0e8bcaa39b2   \n",
       " 5819  33be2a8eb28b541f8edeb0e8bcaa39b2  33be2a8eb28b541f8edeb0e8bcaa39b2   \n",
       " 16    4b71caea13d1b0f5684730a90ecdedf6  4b71caea13d1b0f5684730a90ecdedf6   \n",
       " 17    3ac718e10c17bcbdbb89e33665394c74  3ac718e10c17bcbdbb89e33665394c74   \n",
       " 18    7161e1282a97def92376a4e9088ceafe  7161e1282a97def92376a4e9088ceafe   \n",
       " 19    22a253f736ed290e579b35759ff0943e  22a253f736ed290e579b35759ff0943e   \n",
       " 2670  22a253f736ed290e579b35759ff0943e  22a253f736ed290e579b35759ff0943e   \n",
       " 4613  22a253f736ed290e579b35759ff0943e  22a253f736ed290e579b35759ff0943e   \n",
       " 7775  22a253f736ed290e579b35759ff0943e  22a253f736ed290e579b35759ff0943e   \n",
       " ...                                ...                               ...   \n",
       " 9957  8af0e57b4976cf534e3800fcaf7ed9c2  8af0e57b4976cf534e3800fcaf7ed9c2   \n",
       " 9959  18b8a57d1898532858db8a988331cc47  18b8a57d1898532858db8a988331cc47   \n",
       " 9960  afe4a0b7743b9ac73d24fbae2cbc5462  afe4a0b7743b9ac73d24fbae2cbc5462   \n",
       " 9961  5469a0ce77596faf73c040695cce8751  5469a0ce77596faf73c040695cce8751   \n",
       " 9965  722f416861add3bc1c9b159d929a421c  722f416861add3bc1c9b159d929a421c   \n",
       " 9966  ef6ea064db4e2932b6815732a5df5bca  ef6ea064db4e2932b6815732a5df5bca   \n",
       " 9968  984948d7e62be35521d743ad4114210e  984948d7e62be35521d743ad4114210e   \n",
       " 9970  7c3b9d89c3f3ca537fd763b4016722de  7c3b9d89c3f3ca537fd763b4016722de   \n",
       " 9972  f0529299e7e6cb443559afb0060119d5  f0529299e7e6cb443559afb0060119d5   \n",
       " 9974  503c6354cdf9b3f61d9849ebbe12a481  503c6354cdf9b3f61d9849ebbe12a481   \n",
       " 9975  84ee5258cf0a4bad900ed3daa0384ad3  84ee5258cf0a4bad900ed3daa0384ad3   \n",
       " 9976  9098438b6903f0880e85f2440c499fb2  9098438b6903f0880e85f2440c499fb2   \n",
       " 9977  25d8e9f116e265b8e952a35d232cd7a1  25d8e9f116e265b8e952a35d232cd7a1   \n",
       " 9980  92a6d551623a75fd8e11875e34369390  92a6d551623a75fd8e11875e34369390   \n",
       " 9981  857ebdee03015ea74d4ad7080d4bd569  857ebdee03015ea74d4ad7080d4bd569   \n",
       " 9982  26487bd100fbdf5bc0f4865f11a27a4b  26487bd100fbdf5bc0f4865f11a27a4b   \n",
       " 9983  987f11a3537fd7f933d5c7dc51825deb  987f11a3537fd7f933d5c7dc51825deb   \n",
       " 9984  81a2a7689bf1dd3539bf492c0c1e639e  81a2a7689bf1dd3539bf492c0c1e639e   \n",
       " 9985  80f72c8813d64d6be8a64810d578e14f  80f72c8813d64d6be8a64810d578e14f   \n",
       " 9986  7f25592bf807f43b546e35fc5880bd4d  7f25592bf807f43b546e35fc5880bd4d   \n",
       " 9987  e47f3e289f0ff56000143680bb2ca445  e47f3e289f0ff56000143680bb2ca445   \n",
       " 9989  1a592b143c13f5dfa1e409e4b367640c  1a592b143c13f5dfa1e409e4b367640c   \n",
       " 9990  fc9f215c5128fd92c24fd93aed5aa913  fc9f215c5128fd92c24fd93aed5aa913   \n",
       " 9991  dd35716a3ad3f2ad62746c005b030e44  dd35716a3ad3f2ad62746c005b030e44   \n",
       " 9992  8c12a3fef1060278b762bd8c9fc3902a  8c12a3fef1060278b762bd8c9fc3902a   \n",
       " 9993  34717675ed99df52b66bb2678da6d46a  34717675ed99df52b66bb2678da6d46a   \n",
       " 9994  eb95daaa167a1e572bacd048bc4559dd  eb95daaa167a1e572bacd048bc4559dd   \n",
       " 9995  597f24e55480d8450331d4e1bc663aa1  597f24e55480d8450331d4e1bc663aa1   \n",
       " 9996  515efc319e3bcee86e5c0c6458015981  515efc319e3bcee86e5c0c6458015981   \n",
       " 9997  b3ba7cb42a9a3d3a30a9775f27fe70b4  b3ba7cb42a9a3d3a30a9775f27fe70b4   \n",
       " \n",
       "                                                    body        subreddit  \\\n",
       " 0     Most of us have some family members like this....         exmormon   \n",
       " 1     But Mill's career was way better. Bentham is l...   CanadaPolitics   \n",
       " 2668  &gt;Daily Mail reporting Jermain Defoe to Alph...              tfc   \n",
       " 2     Mine uses a strait razor, and as much as i lov...    AdviceAnimals   \n",
       " 6971  low dps, and lack of utility. People dont like...              wow   \n",
       " 3     The guy is a professional, and very good at wh...              WTF   \n",
       " 4     This is a great question, and I want to thank ...       needadvice   \n",
       " 5     Is the IE-Shiv-Ghostblade-Zerks-LW-BT Still no...   summonerschool   \n",
       " 6     I don't know how to describe it.  Gently pinch...      sausagetalk   \n",
       " 2330  It's a cabelas model.  Can't remember the numb...          Hunting   \n",
       " 8845  Ok.  I have a cheap ass Big Chief smoker.  It'...             food   \n",
       " 7     You mean the village hidden in filler complain...           Naruto   \n",
       " 2691  No. We actually have 55000. Still much more th...           Naruto   \n",
       " 8     If you enjoy deep, 100 hour RPGs, then definit...            Games   \n",
       " 9     Haha awesome man I got it from my grandpa this...           knives   \n",
       " 10    I completely agree. I've spent that long stari...              LSD   \n",
       " 11    &gt;&gt;If a woman wants to give up a child fo...     changemyview   \n",
       " 12    I haven't. Still trying to get someone to comm...        beertrade   \n",
       " 13    It's a religion that doesn't have a set in sto...   TumblrInAction   \n",
       " 5992        Because the heir of Kim Jong Un would know.   Showerthoughts   \n",
       " 14    \"Hey Rocky, Watch me board this train.\" \\n\"Aga...  GrandTheftAutoV   \n",
       " 15    Roofers, the only people on a job site more sa...        AskReddit   \n",
       " 5819  I roofed one house as a favor, and that was en...        AskReddit   \n",
       " 16    You are a gentleman and a scholar...  I don't ...             news   \n",
       " 17    My math prof doesn't do rides, he says \"look a...              WTF   \n",
       " 18       Agreed! You should get it while it's on sale.        HannibalTV   \n",
       " 19    The vast majority of countries in the world re...        worldnews   \n",
       " 2670  Canaanites probably have the oldest claim. Unf...        worldnews   \n",
       " 4613  As in all religions there are moderates and ex...        worldnews   \n",
       " 7775  Forget about Arafat because arguments him alwa...        worldnews   \n",
       " ...                                                 ...              ...   \n",
       " 9957  I started watching it a few weeks ago.  It's f...             cars   \n",
       " 9959  If I get a card I will probably buy a bunch of...     pcmasterrace   \n",
       " 9960  I want DC to win as well but whatever. Just lo...        beertrade   \n",
       " 9961  Thanks for the heads up. Definitely would like...             USMC   \n",
       " 9965  Is this post a fucking joke? Assad has killed ...        worldnews   \n",
       " 9966  In the note she wrote, \"People say 'it gets be...          atheism   \n",
       " 9968  [Kilroy Was Here](http://en.wikipedia.org/wiki...        AskReddit   \n",
       " 9970  A guy brought me his laptop one time saying th...        AskReddit   \n",
       " 9972  followed by \"yourself?\" \\n\\nThe only time it i...        AskReddit   \n",
       " 9974  Can you pull your computer out from behind the...         buildapc   \n",
       " 9975  How do you know so little about the person you...        BabyBumps   \n",
       " 9976  I'd love to see this 'mountain' you are talkin...        worldnews   \n",
       " 9977  heroin and ketamine. they're the only \"big nam...   DarkNetMarkets   \n",
       " 9980  Damn, man I can't wait to see the sick call sl...             army   \n",
       " 9981  I got that router too.  I've enjoyed it a lot ...      woodworking   \n",
       " 9982  Stuff that I try to include in all of my citie...        Minecraft   \n",
       " 9983  Good man. I was afraid I'd suddenly just want ...            funny   \n",
       " 9984  I just gave my friend(Whose family does not al...     pcmasterrace   \n",
       " 9985  Its good to back and watching the rockets. I m...              nba   \n",
       " 9986  More upfront sales from people who haven't pla...            Games   \n",
       " 9987  &gt; community college is only a last resort i...        AskReddit   \n",
       " 9989  I main a prot pally but also have a warrior ta...              wow   \n",
       " 9990  Girl I used to date...she loved it when I'd sn...        AskReddit   \n",
       " 9991  Happy new year Literatewolf! +/u/dogetipbot 50...         dogecoin   \n",
       " 9992  And then they wonder why so many black girls a...      blackladies   \n",
       " 9993              Haha, non. Je n'ai pas cet honneur...           france   \n",
       " 9994               &gt; **FFF**EEeewwweeee^eeee\\n\\nFTFY           videos   \n",
       " 9995  Well this part of California around Big Bear a...          Atlanta   \n",
       " 9996  I had a similar problem. Ended up having to di...   EliteDangerous   \n",
       " 9997  I was going to watch something slightly more u...         Wishlist   \n",
       " \n",
       "      subreddit_id  score author_right  \n",
       " 0        t5_2r0gj   14.0          NaN  \n",
       " 1        t5_2s4gt    3.0          NaN  \n",
       " 2668     t5_2ro5q    2.0          NaN  \n",
       " 2        t5_2s7tt    1.0          NaN  \n",
       " 6971     t5_2qio8    7.0          NaN  \n",
       " 3        t5_2qh61    6.0          NaN  \n",
       " 4        t5_2r367    1.0          NaN  \n",
       " 5        t5_2t9x3    1.0          NaN  \n",
       " 6        t5_2t13q    2.0          NaN  \n",
       " 2330     t5_2qlkx    2.0          NaN  \n",
       " 8845     t5_2qh55    1.0          NaN  \n",
       " 7        t5_2quts    2.0          NaN  \n",
       " 2691     t5_2quts    1.0          NaN  \n",
       " 8        t5_2qhwp    1.0          NaN  \n",
       " 9        t5_2qzyn    1.0          NaN  \n",
       " 10       t5_2qhvj    3.0          NaN  \n",
       " 11       t5_2w2s8   17.0          NaN  \n",
       " 12       t5_2rgco    1.0          NaN  \n",
       " 13       t5_2vizz    2.0          NaN  \n",
       " 5992     t5_2szyo    1.0          NaN  \n",
       " 14       t5_2t0xk    2.0          NaN  \n",
       " 15       t5_2qh1i    2.0          NaN  \n",
       " 5819     t5_2qh1i    2.0          NaN  \n",
       " 16       t5_2qh3l   -1.0          NaN  \n",
       " 17       t5_2qh61    7.0          NaN  \n",
       " 18       t5_2vs7z    1.0          NaN  \n",
       " 19       t5_2qh13    0.0          NaN  \n",
       " 2670     t5_2qh13   -4.0          NaN  \n",
       " 4613     t5_2qh13    9.0          NaN  \n",
       " 7775     t5_2qh13    0.0          NaN  \n",
       " ...           ...    ...          ...  \n",
       " 9957     t5_2qhl2   14.0          NaN  \n",
       " 9959     t5_2sgp1    1.0          NaN  \n",
       " 9960     t5_2rgco    2.0          NaN  \n",
       " 9961     t5_2qswv    1.0          NaN  \n",
       " 9965     t5_2qh13    2.0          NaN  \n",
       " 9966     t5_2qh2p    1.0          NaN  \n",
       " 9968     t5_2qh1i    3.0          NaN  \n",
       " 9970     t5_2qh1i    2.0          NaN  \n",
       " 9972     t5_2qh1i    1.0          NaN  \n",
       " 9974     t5_2rnve    1.0          NaN  \n",
       " 9975     t5_2s7cl   47.0          NaN  \n",
       " 9976     t5_2qh13    3.0          NaN  \n",
       " 9977     t5_2yt0h    0.0          NaN  \n",
       " 9980     t5_2qtr8    3.0          NaN  \n",
       " 9981     t5_2qlqp    1.0          NaN  \n",
       " 9982     t5_2r05i    5.0          NaN  \n",
       " 9983     t5_2qh33    1.0          NaN  \n",
       " 9984     t5_2sgp1    1.0          NaN  \n",
       " 9985     t5_2qo4s    1.0          NaN  \n",
       " 9986     t5_2qhwp    1.0          NaN  \n",
       " 9987     t5_2qh1i    6.0          NaN  \n",
       " 9989     t5_2qio8    1.0          NaN  \n",
       " 9990     t5_2qh1i    1.0          NaN  \n",
       " 9991     t5_2zcp2    2.0          NaN  \n",
       " 9992     t5_2vk9t    8.0          NaN  \n",
       " 9993     t5_2qhjz    5.0          NaN  \n",
       " 9994     t5_2qh1e    0.0          NaN  \n",
       " 9995     t5_2qiq9    3.0          NaN  \n",
       " 9996     t5_2vi60    1.0          NaN  \n",
       " 9997     t5_2qpbu    3.0          NaN  \n",
       " \n",
       " [10000 rows x 7 columns]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neither_author_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "anorexia_author_data = pd.read_csv('data/anorexia_author_data.csv', encoding='ISO-8859-1') \n",
    "obesity_author_data = pd.read_csv('data/obesity_author_data.csv', encoding='ISO-8859-1')\n",
    "neither_author_data = pd.read_csv('data/neither_author_data.csv', encoding='ISO-8859-1')\n",
    "\n",
    "anorexia_author_data.insert(len(anorexia_author_data.columns), 'category', 'anorexia')\n",
    "obesity_author_data.insert(len(obesity_author_data.columns), 'category', 'obesity')\n",
    "neither_author_data.insert(len(neither_author_data.columns), 'category', 'neither')\n",
    "\n",
    "# Each dataset has ~10K rows so split into training and test sets of 5000 rows each.\n",
    "anorexia_author_data_train = anorexia_author_data.head(5000)\n",
    "anorexia_author_data_test = anorexia_author_data.head(5000)\n",
    "obesity_author_data_train = obesity_author_data.head(5000)\n",
    "obesity_author_data_test = obesity_author_data.tail(5000)\n",
    "neither_author_data_train = neither_author_data.head(5000)\n",
    "neither_author_data_test = neither_author_data.tail(5000)\n",
    "\n",
    "train_data = pd.concat([anorexia_author_data_train, obesity_author_data_train, neither_author_data_train])\n",
    "test_data = pd.concat([anorexia_author_data_test, obesity_author_data_test, neither_author_data_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature extraction/Model selection pipeline\n",
    "\n",
    "Based heavily on:\n",
    "*   http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "*   http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 188.983s\n",
      "\n",
      "Best score: 0.423\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__ngram_range: (1, 2)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   anorexia       0.63      0.92      0.75      5000\n",
      "    neither       0.62      0.51      0.56      5000\n",
      "    obesity       0.52      0.38      0.44      5000\n",
      "\n",
      "avg / total       0.59      0.60      0.58     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(train_data['body'].values.tolist(), \n",
    "                    train_data['category'].values.tolist())\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters_a = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters_a[param_name]))\n",
    "    \n",
    "    y_true, y_pred = test_data['category'].values.tolist(), grid_search.predict(test_data['body'].values.tolist()) \n",
    "    \n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
